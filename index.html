<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-QC2GQV1130"></script>
    <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-QC2GQV1130');



    </script>

    <title>Jishnu Ray Chowdhury</title>

    <meta name="author" content="Jishnu Ray Chowdhury">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <!-- Intro -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Jishnu Ray Chowdhury</name>
                        </p>
                        <p>
                            I am a PhD candidate at the University of Illinois at Chicago (UIC), working with Prof. <a
                                href="https://www.cs.uic.edu/~cornelia/">Cornelia Caragea</a>.
                            During the summer of 2020 and 2021, I did research internships at <strong>Bloomberg</strong>.
                            <br><br>
                            I explored ways to extend Recursive Neural Networks (RvNN) for length generalization and
                            compositional generalization
                            without access to ground truth structure data. I developed Continuous Recursive Neural
                            Networks (CRvNN)
                            which softens the structure and order of composition - making it closer to Transformers. I
                            also extended
                            Gumbel Tree RvNNs with Beam Search and introduced several tricks to reduce memory
                            consumption by more than ten times. I also explored ways to interface RvNNs
                            with Transformers. I have also worked on novel forms of location attention for length
                            generalization using Seq2Seq models.
                            I have done work on prompt tuning, prompt engineering, contrastive learning, imitation
                            learning, keyphrase generation, and
                            question generation.
                        </p>
                        <p style="text-align:center">
                            <a href="mailto:jishnu.ray.c@gmail.com">Email</a> &nbsp/&nbsp
                            <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                            <a href="https://github.com/JRC1995">GitHub</a> &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=aA6BAS0AAAAJ&hl=en">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://www.linkedin.com/in/jrc1995/">LinkedIn</a>
                        </p>
                        <p>
                            <strong>Contents:</strong>
                        <ul>
                            <li><a href="#research">Research</a></li>
                            <li><a href="#projects">Projects</a></li>
                            <li><a href="#services">Services</a></li>
                        </ul>
                        (Last updated: 2/7/2024)
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.png"
                             class="hoverZoomLink">
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Publications title -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading id="research" style="color:#0066cc;">Research</heading>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <strong>Recurrent/Recursive Neural Networks</strong>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- List of publications -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/UT.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2402.00976">
                            <papertitle>Recurrent Transformers with Dynamic Halt</papertitle>
                        </a>
                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea
                        <br>
                        ArXiv, 2024
                        <br>
                        <a href="https://arxiv.org/abs/2402.00976">pdf</a>
                        <p>
                            We empirically study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/rir.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2311.04449">
                        <papertitle>Recursion in Recursion</papertitle>
                        </a>
                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea
                        <br>
                        NeurIPS, 2023
                        <br>
                        <a href="https://arxiv.org/pdf/2311.04449.pdf">pdf</a>/
                        <a href="https://github.com/JRC1995/BeamRecursionFamily/">code</a>
                        <p>
                            We implement a Balanced Tree Recursion at a chunk level. Each chunk at any iteration is
                            processed by Efficient Beam Tree Recursive Neural Networks (EBT-RvNN).
                            Thus, Recursion within a Recursion is implemented. This hybrid setup is much more
                            computationally efficient than EBT-RvNN. On the other hand, it still performs
                            competitively in ListOps length generalization and logical inference, unlike fully Balanced
                            Tree models or other models like Transformers/SSMs. The model setup also performs well in the
                            text-related tasks of LRA.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/EBT-RvNN.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2307.10779">
                            <papertitle>Efficient Beam Tree Recursion</papertitle>
                        </a>
                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea
                        <br>
                        NeurIPS, 2023
                        <br>
                        <a href="https://arxiv.org/pdf/2307.10779.pdf">pdf</a>/
                        <a href="https://github.com/JRC1995/BeamRecursionFamily/">code</a>
                        <p>
                            Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of
                            Gumbel Tree RvNN. We identify a memory bottleneck in it and remove it
                            leading to 10-16 times less memory consumption.
                            In addition, we also propose a strategy to utilize the induced latent-tree node
                            representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder
                            into a sequence contextualizer by sending top-down signals from the parent node to leaf nodes
                            using attention. This opens up a way to interface BT-RvNN with other
                            downstream modules like Transformers.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/BT-RvNN.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2305.19999">
                            <papertitle>Beam Tree Recursive Cells</papertitle>
                        </a>
                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea
                        <br>
                        ICML, 2023
                        <br>
                        <a href="https://arxiv.org/pdf/2305.19999.pdf">pdf</a> /
                        <a href="https://github.com/JRC1995/BeamTreeRecursiveCells">code</a>
                        <p>
                            We extend Gumbel Tree LSTM by replacing Gumbel softmax with a soft top-k mechanism and use
                            it for beam search instead of greedy easy-first parsing.
                            This simple method performs competitively with other more sophisticated implementations of
                            RvNNs. Our proposed soft top-k mechanism can be explored
                            further for other tasks where sending gradient signals through some top-k is deemed
                            important.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/CRvNN.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="http://proceedings.mlr.press/v139/chowdhury21a.html">
                            <papertitle>Modeling Hierarchical Structures with Continuous Recursive Neural Networks
                            </papertitle>
                        </a>
                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea
                        <br>
                        ICML <strong>(Long Talk)</strong>, 2021
                        <br>
                        <a href="http://proceedings.mlr.press/v139/chowdhury21a/chowdhury21a.pdf">pdf</a> /
                        <a href="https://github.com/JRC1995/Continuous-RvNN">code</a> /
                        <a href="https://slideslive.com/38958572/modeling-hierarchical-structures-with-continuous-recursive-neural-networks?ref=recommended">Talk</a>
                        <p>
                            We propose Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly implementation of RvNNs without surrogate gradients, reinforcement learning, or stack-augmented recurrent operations.
                            This is done by incorporating a continuous relaxation to the induced structure.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <strong>OOD-Generalization, Length Generalization</strong>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/monotonic.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://proceedings.mlr.press/v202/ray-chowdhury23b.html">
                            <papertitle>Monotonic Location Attention for Length Generalization</papertitle>
                        </a>

                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea
                        <br>
                        ICML, 2023
                        <br>
                        <a href="https://proceedings.mlr.press/v202/ray-chowdhury23b/ray-chowdhury23b.pdf">pdf</a> /
                        <a href="https://github.com/JRC1995/MonotonicLocationAttention">code</a>
                        <p>
                            We explore different ways to utilize position-based cross-attention in seq2seq networks to
                            enable length generalization in algorithmic tasks.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <strong>Prompt Tuning/Engineering</strong>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/rapt.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21297">
                            <papertitle>Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional
                                Prompt Tuning
                            </papertitle>
                        </a>
                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Yong Zhuang,
                        Shuyi Wang
                        <br>
                        AAAI <strong>(Oral)</strong>, 2022
                        <br>
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21297/21046">pdf</a>
                        <p>
                            We concentrate on two contributions to paraphrase generation:
                            (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to
                            adapt large pre-trained language models for paraphrase generation;
                            (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using
                            specialized prompt tokens for controlled paraphrase generation
                            with varying levels of lexical novelty. 
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <strong>Keyphrase Generation</strong>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/kpdrop.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://aclanthology.org/2022.findings-emnlp.357/">
                            <papertitle>KPDROP: Improving Absent Keyphrase Generation</papertitle>
                        </a>

                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Seo Yeon Park *,
                        Tuhin Kundu *,
                        Cornelia Caragea
                        <br>
                        EMNLP Findings, 2023
                        <br>
                        <a href="https://aclanthology.org/2022.findings-emnlp.357.pdf">pdf</a> /
                        <a href="https://github.com/JRC1995/KPDrop">code</a>
                        <p>
                            We propose a model-agnostic approach called keyphrase dropout (or KPDrop) to improve absent
                            keyphrase generation. 
                            In this approach, we all instances of some randomly chosen present keyphrases from the document and turn them into
                            artificial absent keyphrases during training. We also explore the benefits of this method in a semi-supervised training regime.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/keyphrase.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2304.13883">
                            <papertitle>Neural Keyphrase Generation: Analysis and Evaluation</papertitle>
                        </a>
                        <br>
                        Tuhin Kundu,
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea
                        <br>
                        ArXiv 2023
                        <br>
                        <a href="https://arxiv.org/pdf/2304.13883.pdf">pdf</a>
                        <p>
                            we study various tendencies exhibited by three strong models: T5 (based on a
                            pre-trained transformer), CatSeq-Transformer
                            (a non-pretrained Transformer), and ExHiRD
                            (based on a recurrent neural network). We analyze prediction confidence scores, model
                            calibration, and the effect of token position on
                            keyphrases generation. Moreover, we motivate and propose a novel metric framework,
                            SoftKeyScore, to evaluate the similarity between two sets of keyphrases by using softscores
                            to account for partial matching and semantic similarity
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/aug.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://aclanthology.org/2023.findings-acl.534/">
                            <papertitle>Data Augmentation for Low-Resource Keyphrase Generation</papertitle>
                        </a>

                        <br>
                        Krishna Garg
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea
                        <br>
                        ACL Findings, 2023
                        <br>
                        <a href="https://aclanthology.org/2023.findings-acl.534.pdf">pdf</a> /
                        <a href="https://github.com/kgarg8/kpgen-lowres-data-aug">code</a>
                        <p>
                            We present data augmentation strategies specifically to address keyphrase generation in
                            purely
                            resource-constrained domains. We design techniques that use the full text of the articles to
                            improve both present and absent keyphrase
                            generation
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/boundaries.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://aclanthology.org/2022.findings-emnlp.427/">
                            <papertitle>Keyphrase Generation Beyond the Boundaries of Title and Abstract
                            </papertitle>
                        </a>

                        <br>
                        Krishna Garg
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea
                        <br>
                        EMNLP Findings, 2022
                        <br>
                        <a href="https://aclanthology.org/2022.findings-emnlp.427.pdf">pdf</a> /
                        <a href="https://github.com/kgarg8/FullTextKP">code</a>
                        <p>
                            We comprehensively
                            explore whether the integration of additional
                            information from the full text of a given article or from semantically similar articles can
                            be helpful for a neural keyphrase generation
                            model or not. We discover that adding sentences from the full text, particularly in the
                            form of the extractive summary of the article can significantly improve the generation
                            of both types of keyphrases that are either
                            present or absent from the text.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <strong>Question Generation</strong>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/control_flow.png" width="160" , height="180" style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2203.04464">
                            <papertitle>On the Evaluation of Answer-Agnostic Paragraph-level Multi-Question Generation
                            </papertitle>
                        </a>

                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea, Debanjan Mahata
                        <br>
                        ArXiv 2022
                        <br>
                        <a href="https://arxiv.org/pdf/2203.04464.pdf">pdf</a> /
                        <a href="https://github.com/JRC1995/QuestionGenerationPub">code</a>
                        <p>
                            We study the task of predicting a set of salient questions from a given paragraph without
                            any prior knowledge of the precise answer. We make two main contributions. First, we propose
                            a new method to evaluate a set of predicted questions against the set of references by using
                            the Hungarian algorithm to assign predicted questions to references before scoring the
                            assigned pairs. We show that our proposed evaluation strategy has better theoretical and
                            practical properties compared to prior methods because it can properly account for the
                            coverage of references.
                            Second, we compare different strategies to utilize a pre-trained seq2seq model to generate
                            and select a set of questions related to a given paragraph.
                        </p>
                    </td>
                </tr>

                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <strong>Disaster-Related Information Extraction</strong>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/disaster.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://aclanthology.org/2020.acl-srw.39/">
                            <papertitle>Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold
                                Mixup
                            </papertitle>
                        </a>

                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea,
                        Doina Caragea
                        <br>
                        ACL SRW, 2020
                        <br>
                        <a href="https://aclanthology.org/2020.acl-srw.39.pdf">pdf</a> /
                        <a href="https://github.com/JRC1995/Multilingual-BERT-Disaster">code</a>
                        <p>
                            We present a masking-based loss function for partially labeled samples and demonstrate the
                            effectiveness of Manifold Mixup in the text domain. Our main model is based on Multilingual
                            BERT, which we further improve with Manifold Mixup. We show that our model generalizes to
                            unseen disasters in the test set. Furthermore, we analyze the capability of our model for
                            zero-shot generalization to new languages.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/disastergen.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5387">
                            <papertitle>On Identifying Hashtags in Disaster Twitter Data
                            </papertitle>
                        </a>

                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea, Doina Caragea
                        <br>
                        AAAI (Special Track, AI for Social Impact), 2020
                        <br>
                        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/5387/5243">pdf</a> /
                        <a href="https://github.com/JRC1995/Tweet-Disaster-Keyphrase">code</a>
                        <p>
                            To facilitate progress on automatic identification (or extraction) of disaster hashtags for
                            Twitter data, we construct a unique dataset of disaster-related tweets annotated with
                            hashtags useful for filtering actionable information. Using this dataset, we further
                            investigate Long Short-Term Memory-based models within a Multi-Task Learning framework.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/disastergen.png" width="160" , style="padding-top: 7mm;">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://dl.acm.org/doi/10.1145/3308558.3313696">
                            <papertitle>Keyphrase Extraction from Disaster-related Tweets
                            </papertitle>
                        </a>

                        <br>
                        <strong>Jishnu Ray Chowdhury</strong>,
                        Cornelia Caragea, Doina Caragea
                        <br>
                        WWW, 2019
                        <br>
                        <a href="https://dl.acm.org/doi/pdf/10.1145/3308558.3313696?casa_token=UsSr1XchMZgAAAAA:bP2CxkZfMKrowF_qa7e2Af7ZaNAk9uoRU84xlr1zIWrcBqIwTrYhQxvzCLq4YQNRm5XRWpg7CmIfxQ">pdf</a>
                        <p>
                            We explore keyphrase extraction models for extracting disaster-related keyphrases from tweets.
                            We employ a joint-training-based approach (for keyword discovery and keyphrase extraction). 
                            We extend it using contextual word embeddings, POS-tags, phonetics, and phonological features. We also propose an embedding-based metrics to better capture the correctness
                            of the predicted keyphrases.
                        </p>
                    </td>
                </tr>

                </tbody>
            </table>


            <!-- Workshop papers title -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading id="projects" style="color:#0066cc;">Projects</heading>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/zeroprompt.jpg" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://github.com/JRC1995/ZeroPromptSearch">
                            <papertitle>Zero-Shot Prompts for Step Decomposition and Search (2023)
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/JRC1995/ZeroPromptSearch">code</a>
                        <br>
                        <p>
                        Implementation of an LLM prompting pipeline combined with wrappers for auto-decomposing reasoning steps and for search through the reasoning step space (eg. by beam search, MCTS etc.) 
                            guided by self-evaluation rewards.
                        </p>
                    </td>
                </tr>    
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/Chatbot.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://github.com/JRC1995/Chatbot">
                            <papertitle>Open-Domain Conversational AI with Hybrid Generative and Retrieval Mechanisms
                                (2020)
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/JRC1995/Chatbot/blob/master/Project%20Report.pdf">pdf</a> /
                        <a href="https://github.com/JRC1995/Chatbot">code</a> /
                        <a href="https://www.youtube.com/embed/Zbz-vEp5coo">video</a>
                        <br>
                        <p>
                            This is a pre-ChatGPT era model. It uses Dialog-GPT combined with a response retrieval mechanism
                            from a custom script (can be used for customing personality) and Reddit database. The overall model is a synergy of multiple
                            sub-modules for retrieval, dialog classification, generation, and ranking. It also incorporates a
                            Text-to-Speech Synthesis Mechanism.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/opt.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://github.com/JRC1995/DemonRangerOptimizer">
                            <papertitle>Experimental Optimizer Library (2021)</papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/JRC1995/DemonRangerOptimizer">code</a>
                        <br>
                        <p>
                            The library allows synergy of different optimization strategies like hypergradient
                            optimization, nostalgia, variance rectification, lookahead, decaying momentum, iterate
                            averaging, gradient checkpointing, gradient noise, quasi-hyperbolic momentum, decorrelated
                            weight decay, and more.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/ner.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://github.com/JRC1995/SocialMediaNER">
                            <papertitle>Named Entity Recognition on Social Media (2021)</papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/JRC1995/SocialMediaNER/blob/main/Named_Entity_Recognition_in_Social_Media.pdf">pdf</a>
                        /
                        <a href="https://github.com/JRC1995/SocialMediaNER">code</a>
                        <br>
                        <p>
                            We address the challenges posed by noise and emerging/rare entities in Named Entity
                            Recognition task for social media domain. Following the recent advances, we employ
                            Contextualized Word Embeddings from Language Models pre-trained on large corpora along with
                            some normalization techniques to reduce noise. Our best model achieves state-of-the-art (at the time of the project) results (F1 52.47%) on WNUT 2017 dataset. Additionally, we adopt a modular approach to
                            systematically evaluate different contextual embeddings and downstream labeling mechanisms
                            using Sequence Labeling and a Question Answering framework.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/capsule.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://github.com/JRC1995/CapsuleRoutingEncoders">
                            <papertitle>Text Classification with Capsule Routing (2021)</papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/JRC1995/CapsuleRoutingEncoders/blob/main/CS_521_Project_report.pdf">pdf</a>
                        /
                        <a href="https://github.com/JRC1995/CapsuleRoutingEncoders">code</a>
                        <br>
                        <p>
                            In this work, I study and compare multiple capsule routing algorithms for text
                            classification, including dynamic routing, Heinsen routing, and capsule-routing-inspired
                            attention-based sentence encoding techniques like dynamic self-attention. We analyze the theoretical connection between attention
                            and capsule routing and contrast the two ways of normalizing the routing weights. Finally,
                            I present a new way to do capsule routing, or rather an iterative refinement, using a richer
                            attention function to measure agreement among output and input capsules and with highway
                            connections in between iterations.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/covariate.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://github.com/JRC1995/Causal-Inference">
                            <papertitle>Exploring Disentanglement and Invariance in the Face of Co-variate Shift
                                (2019)
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/JRC1995/CapsuleRoutingEncoders/blob/main/CS_521_Project_report.pdf">pdf</a>
                        /
                        <a href="https://github.com/JRC1995/Causal-Inference">code</a>
                        <br>
                        <p>
                            Despite the strong predictive performance of Machine Learning models, they are still subject
                            to spurious correlations. Working under the I.I.D. assumption, they are often hard to
                            generalize to out-of-distribution test data. In this project, we tackle the problem of
                            co-variate shift, where the test data is from a different distribution than the training
                            data. Particularly, we approach this problem from a causal framework. We compare IRMv1,
                            CoRe, ICP, and Entropy Penalty (EP) on different settings. Furthermore, we experiment with
                            disentangled representations, and we try to enhance classification results by gating the
                            features of intermediate hidden state representations based on their influence on the
                            classification probabilities. 
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/sparse.jpg" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://github.com/JRC1995/Virtues-of-Sparsity">
                            <papertitle>Neural Network Sparsification (2019)</papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/JRC1995/Virtues-of-Sparsity">code</a>
                        <br>
                        <p>
                            Experiments with continuous sparsification of neural network connections and sparse
                            representation (using K-winner activation function) on NLP tasks like Named Entity
                            Recognition. Continuous sparsification could halve the number of parameters without any
                            significant F1 loss.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/edialectics.jpg" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://github.com/JRC1995/Faux-Reddit">
                            <papertitle>E-Dialectics - Reddit-like Toy Website (Database Project) (2019)</papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/JRC1995/Faux-Reddit">code</a>
                        <br>
                        <p>
                            The project is inspired by Reddit and the initial data is populated from a sample of real
                            Reddit data that was retrieved using Google Big Query. The database is implemented using
                            both SQL and NoSQL technologies in a complementary fashion. Users of E-Dialectics can read
                            discussion threads and the comments for them, which are created by other users; each thread
                            can also be under different subforums (eg. philosophy, computer science, science etc.).
                            Anyone can read the threads and comments, but to be able to interact further they must log in
                            to the site; new users can create their own accounts. Users can then create new threads and
                            comments after they log into their accounts.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/summary.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>TextRank and RAKE (2017)</papertitle>
                        <br>
                        <p>
                            Unsupervised extractive summarization using RAKE (<a
                                href="https://github.com/JRC1995/auto-tldr-RAKE">code</a>).
                            Unsupervised keyphrase extraction using RAKE (<a
                                href="https://github.com/JRC1995/RAKE-Keyword-Extraction">code</a>).
                            Unsupervised extractive summarization using TextRank (<a
                                href="https://github.com/JRC1995/auto-tldr-TextRank">code</a>).
                            Unsupervised keyphrase extraction using TextRank (<a
                                href="https://github.com/JRC1995/TextRank-Keyword-Extraction">code</a>).
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/summary.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Abstractive Summarization and Machine Translation (2017)</papertitle>
                        <br>
                        <p>
                            Abstractive Summarization with RNNs (<a
                                href="https://github.com/JRC1995/Abstractive-Summarization">code</a>).
                            Machine Translation using Transformers (<a
                                href="https://github.com/JRC1995/Machine-Translation-Transformers">code</a>).
                            Abstractive Summarization with intra-attention-based LSTM encoder (<a
                                href="https://github.com/JRC1995/INTER-INTRA-attentions">code</a>).
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/qa.jpg" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Question Answering (2017)</papertitle>
                        <br>
                        <p>
                            DMN+ (<a href="https://github.com/JRC1995/Dynamic-Memory-Network-Plus">code</a>).
                            R-NET (<a href="https://github.com/JRC1995/Machine-Translation-Transformers">code</a>).
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/image.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://github.com/JRC1995/Wide-Residual-Network">
                            <papertitle>Image Classification (2017)</papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/JRC1995/Wide-Residual-Network">code</a>
                        <br>
                        <p>
                            Experiment with Wide-ResNet and ResNeXt on CIFAR10 for image classification. Final Year
                            project for Bachelors.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/som.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>Clustering (2017)</papertitle>
                        <br>
                        <p>
                            Self-Organizing-Map (SOM) (<a href="https://github.com/JRC1995/Self-Organizing-Map">code</a>).
                            Fuzzy C Means (<a href="https://github.com/JRC1995/Fuzzy-C-Mean">code</a>).
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/menu_management.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://rbicsolutions.ca/">
                            <papertitle>Menu Management Module (2017)</papertitle>
                        </a>
                        <br>
                        <p> 
                            A full-stack software connected to a database that allows restaurants to manage menus, recipes, ingredients, and food costs among other information. 
                            This was a freelance project. I engaged in full-stack development including design of database, queries, frontend UI design, and some backend programming.
                        </p>
                    </td>
                </tr>

                </tbody>
            </table>

            <!-- Talks -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading id="services" style="color:#0066cc;">Services</heading>
                        </br>
                        </br>
                        <ul>
                            <li>ICML 2024 Reviewer</li>
                            <li>NAACL 2024 Reviewer</li>
                            <li>ICLR 2024 Reviewer</li>
                            <li>AAAI 2024 Reviewer</li>
                            <li>EMNLP 2023 Reviewer</li>
                            <li>NeurIPS 2023 Reviewer</li>
                            <li>ACL 2023 Reviewer</li>
                            <li>ICLR 2023 Reviewer</li>
                            <li>AAAI 2023 Reviewer</li>
                            <li>Multiple ARR Reviews</li>
                            <li>ACL 2021 Reviewer</li>
                        </ul>
                    </td>
                </tr>
            </table>
        </td>
    </tr>
    </tbody>
</table>

<!-- Credits -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr>
        <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
                Website template credits to <a href="https://jonbarron.info">Jon Barron</a> and <a
                    href="https://github.com/RobertCsordas/robertcsordas.github.io/tree/573fe6a078854061b876950d58e496e651a1d118">Rbert
                Csords</a> .
            </p>
        </td>
    </tr>
    </tbody>
</table>
</body>

</html>
